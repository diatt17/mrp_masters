{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f14b8616-ae1c-4191-ba8b-3f3ac50447f7",
   "metadata": {},
   "source": [
    "# Experimentation and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95904abf-f705-4e53-8bce-a6c3ecb6d93e",
   "metadata": {},
   "source": [
    "## Objective of the project \n",
    "\n",
    "This study seeks to conduct a thorough comparative analysis of these three models, focusing\n",
    "on their performance with regards to accuracy, computational complexity, scalability, and their\n",
    "effectiveness in handling data sparsity and dynamically changing environments. By evaluat-\n",
    "ing these aspects, the research aims to illuminate the operational strengths and weaknesses\n",
    "of each model, providing clear insights that could guide the development and deployment of\n",
    "future recommender systems. Through this comparative framework, we aspire to answer which\n",
    "model, under what conditions, provides the most reliable and robust recommendations, thereby\n",
    "significantly contributing to the optimization of digital services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec1c5afc-8ec3-4beb-9777-ef5cd6260e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter, defaultdict\n",
    "from surprise import Dataset, Reader, KNNBasic, SVD, CoClustering, accuracy\n",
    "#from surprise.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from surprise.accuracy import rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d2f34e-8624-4280-9fb9-28ed84c78540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Links':    movieId  imdbId   tmdbId\n",
       " 0        1  114709    862.0\n",
       " 1        2  113497   8844.0\n",
       " 2        3  113228  15602.0\n",
       " 3        4  114885  31357.0\n",
       " 4        5  113041  11862.0,\n",
       " 'Movies':    movieId                               title  \\\n",
       " 0        1                    Toy Story (1995)   \n",
       " 1        2                      Jumanji (1995)   \n",
       " 2        3             Grumpier Old Men (1995)   \n",
       " 3        4            Waiting to Exhale (1995)   \n",
       " 4        5  Father of the Bride Part II (1995)   \n",
       " \n",
       "                                         genres  \n",
       " 0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       " 1                   Adventure|Children|Fantasy  \n",
       " 2                               Comedy|Romance  \n",
       " 3                         Comedy|Drama|Romance  \n",
       " 4                                       Comedy  ,\n",
       " 'Ratings':    userId  movieId  rating  timestamp\n",
       " 0       1        1     4.0  964982703\n",
       " 1       1        3     4.0  964981247\n",
       " 2       1        6     4.0  964982224\n",
       " 3       1       47     5.0  964983815\n",
       " 4       1       50     5.0  964982931,\n",
       " 'Tags':    userId  movieId              tag   timestamp\n",
       " 0       2    60756            funny  1445714994\n",
       " 1       2    60756  Highly quotable  1445714996\n",
       " 2       2    60756     will ferrell  1445714992\n",
       " 3       2    89774     Boxing story  1445715207\n",
       " 4       2    89774              MMA  1445715200}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_df = pd.read_csv('MovieLens_100k/links.csv')\n",
    "movies_df = pd.read_csv('MovieLens_100k/movies.csv')\n",
    "ratings_df = pd.read_csv('MovieLens_100k/ratings.csv')\n",
    "tags_df = pd.read_csv('MovieLens_100k/tags.csv')\n",
    "\n",
    "datasets = {\n",
    "    \"Links\": links_df,\n",
    "    \"Movies\": movies_df,\n",
    "    \"Ratings\": ratings_df,\n",
    "    \"Tags\": tags_df\n",
    "}\n",
    "\n",
    "datasets_info = {name: df.head() for name, df in datasets.items()}\n",
    "datasets_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffed4e8-5b41-485b-8277-e2a6fba0e61b",
   "metadata": {},
   "source": [
    "## Dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "862584cf-a7e4-415e-b6bc-ec7f894fb287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Links dataset:\n",
      "movieId    0\n",
      "imdbId     0\n",
      "tmdbId     8\n",
      "dtype: int64\n",
      "\n",
      "Missing values in Movies dataset:\n",
      "movieId    0\n",
      "title      0\n",
      "genres     0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in Ratings dataset:\n",
      "userId       0\n",
      "movieId      0\n",
      "rating       0\n",
      "timestamp    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in Tags dataset:\n",
      "userId       0\n",
      "movieId      0\n",
      "tag          0\n",
      "timestamp    0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each dataset\n",
    "missing_values = {name: df.isnull().sum() for name, df in datasets.items()}\n",
    "\n",
    "# Print the information about missing values\n",
    "for name, missing in missing_values.items():\n",
    "    print(f\"Missing values in {name} dataset:\\n{missing}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "713549fa-4fca-4d77-9bb7-7d1f73ef073d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the Links DataFrame is: (9742, 3)\n",
      "The shape of the Movies DataFrame is: (9742, 3)\n",
      "The shape of the Ratings DataFrame is: (100836, 4)\n",
      "The shape of the Tags DataFrame is: (3683, 4)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of each DataFrame\n",
    "for name, df in datasets.items():\n",
    "    print(f\"The shape of the {name} DataFrame is: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "083bc746-5ced-4d3e-a934-90d141950572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.5</td>\n",
       "      <td>5550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>20047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.5</td>\n",
       "      <td>13136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.0</td>\n",
       "      <td>26818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.5</td>\n",
       "      <td>8551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>13211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  count\n",
       "0     0.5   1370\n",
       "1     1.0   2811\n",
       "2     1.5   1791\n",
       "3     2.0   7551\n",
       "4     2.5   5550\n",
       "5     3.0  20047\n",
       "6     3.5  13136\n",
       "7     4.0  26818\n",
       "8     4.5   8551\n",
       "9     5.0  13211"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_of_ratings = ratings_df.groupby('rating').size().reset_index(name='count')\n",
    "distribution_of_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc7c59-ca00-4e09-ba88-9ba202175e27",
   "metadata": {},
   "source": [
    "# Hypergraph-based Models: HyperGCN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c4c2303-3f02-44bb-bb44-4197eae74aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 07:23:14.799790: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypergraph created with 9593 nodes and 26961 edges.\n",
      "Number of nodes in hypergraph: 9593\n",
      "Sample nodes: ['user_509.0', 'movie_7347.0', 'user_380.0', 'user_274.0', 'user_474.0']\n",
      "Adjacency matrix created with shape (9593, 9593)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import hypernetx as hnx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "ratings_df = pd.read_csv('MovieLens_100k/ratings.csv')\n",
    "movies_df = pd.read_csv('MovieLens_100k/movies.csv')\n",
    "\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.20, random_state=42)\n",
    "\n",
    "# Build the hypergraph\n",
    "edges = defaultdict(list)\n",
    "for _, row in train_df.iterrows():\n",
    "    user_node = f'user_{row[\"userId\"]}'\n",
    "    movie_node = f'movie_{row[\"movieId\"]}'\n",
    "    rating = row[\"rating\"]\n",
    "    hyperedge = f'{movie_node}_rating_{rating}'\n",
    "    edges[hyperedge].append(user_node)\n",
    "    edges[hyperedge].append(movie_node)\n",
    "\n",
    "H = hnx.Hypergraph(edges)\n",
    "print(f\"Hypergraph created with {len(H.nodes)} nodes and {len(H.edges)} edges.\")\n",
    "print(f\"Number of nodes in hypergraph: {len(H.nodes)}\")\n",
    "print(f\"Sample nodes: {list(H.nodes)[:5]}\")\n",
    "\n",
    "\n",
    "# Create adjacency matrix for hypergraph\n",
    "def create_hypergraph_adjacency_matrix(hypergraph):\n",
    "    node_list = list(hypergraph.nodes)\n",
    "    node_idx = {node: idx for idx, node in enumerate(node_list)}\n",
    "    n = len(node_list)\n",
    "    \n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "\n",
    "    for edge in hypergraph.edges:\n",
    "        edge_nodes = list(hypergraph.edges[edge])\n",
    "        for i in range(len(edge_nodes)):\n",
    "            for j in range(i + 1, len(edge_nodes)):\n",
    "                node_i = node_idx[edge_nodes[i]]\n",
    "                node_j = node_idx[edge_nodes[j]]\n",
    "                data.append(1)\n",
    "                row.append(node_i)\n",
    "                col.append(node_j)\n",
    "                data.append(1)\n",
    "                row.append(node_j)\n",
    "                col.append(node_i)\n",
    "\n",
    "    adj_matrix = csr_matrix((data, (row, col)), shape=(n, n))\n",
    "    print(f\"Adjacency matrix created with shape {adj_matrix.shape}\")\n",
    "    return adj_matrix, node_idx\n",
    "\n",
    "adj_matrix, node_to_idx = create_hypergraph_adjacency_matrix(H)\n",
    "adj_matrix_sparse = tf.sparse.SparseTensor(indices=np.array([adj_matrix.nonzero()[0], adj_matrix.nonzero()[1]]).T,\n",
    "                                           values=adj_matrix.data.astype(np.float32),\n",
    "                                           dense_shape=adj_matrix.shape)\n",
    "adj_matrix_sparse = tf.sparse.reorder(adj_matrix_sparse)\n",
    "\n",
    "\n",
    "# Evaluation metrics functions\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "    return precisions, recalls\n",
    "\n",
    "def compute_mse(predictions):\n",
    "    \"\"\"Compute Mean Squared Error (MSE).\"\"\"\n",
    "    mse = np.mean([(true_r - est) ** 2 for (_, _, true_r, est, _) in predictions])\n",
    "    return mse\n",
    "\n",
    "def compute_rmse(predictions):\n",
    "    \"\"\"Compute Root Mean Squared Error (RMSE).\"\"\"\n",
    "    mse = compute_mse(predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def compute_mae(predictions):\n",
    "    mae = np.mean([abs(true_r - est) for (_, _, true_r, est, _) in predictions])\n",
    "    return mae\n",
    "\n",
    "# Functions to generate sparse and new user data\n",
    "def get_sparse_data(ratings, frac=0.1):\n",
    "    sparse_ratings_df = ratings.sample(frac=frac, random_state=42) \n",
    "    return sparse_ratings_df\n",
    "\n",
    "def get_new_user_data(ratings, frac=0.1):\n",
    "    new_user_ratings_df = ratings[ratings['userId'].isin(ratings['userId'].sample(frac=frac, random_state=42))]\n",
    "    return new_user_ratings_df\n",
    "\n",
    "def evaluate_model(test, embeddings, user_mapping, movie_mapping, scenario, algorithm):\n",
    "    def predict_rating(user, movie):\n",
    "        if user in user_mapping and movie in movie_mapping:\n",
    "            user_idx = user_mapping[user]\n",
    "            movie_idx = movie_mapping[movie]\n",
    "            if user_idx >= embeddings.shape[0] or movie_idx >= embeddings.shape[0]:\n",
    "                return 0\n",
    "            user_emb = embeddings[user_idx]\n",
    "            movie_emb = embeddings[movie_idx]\n",
    "            return np.dot(user_emb, movie_emb)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    predictions = []\n",
    "    for _, row in test.iterrows():\n",
    "        uid = row['userId']\n",
    "        mid = row['movieId']\n",
    "        true_r = row['rating']\n",
    "        est = predict_rating(uid, mid)\n",
    "        predictions.append((uid, mid, true_r, est, None))\n",
    "\n",
    "    mse = compute_mse(predictions)\n",
    "    rmse = compute_rmse(predictions)\n",
    "    mae = compute_mae(predictions)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=10)\n",
    "\n",
    "    avg_precision = np.mean(list(precisions.values()))\n",
    "    avg_recall = np.mean(list(recalls.values()))\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        'Scenario': [scenario],\n",
    "        'Algorithm': [algorithm],\n",
    "        'MSE':[mse],\n",
    "        'RMSE': [rmse],\n",
    "        'MAE': [mae],\n",
    "        'Precision@10': [avg_precision],\n",
    "        'Recall@10': [avg_recall]\n",
    "    })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f808172-5ced-4177-87fc-f559bddcd6a3",
   "metadata": {},
   "source": [
    "# HyperGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a01b9f0b-0130-4097-bf3b-f00c122679a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 80668 negative samples\n",
      "Number of positive samples: 80668\n",
      "Number of negative samples: 80668\n",
      "Epoch 0, Loss: 1.0000461339950562\n",
      "Epoch 10, Loss: 0.9871677160263062\n",
      "Epoch 20, Loss: 0.9546469449996948\n",
      "Epoch 30, Loss: 0.826619029045105\n",
      "Epoch 40, Loss: 0.4226799011230469\n",
      "Epoch 50, Loss: 0.19554905593395233\n",
      "Epoch 60, Loss: 0.11421488225460052\n",
      "Epoch 70, Loss: 0.06767608225345612\n",
      "Epoch 80, Loss: 0.039076920598745346\n",
      "Epoch 90, Loss: 0.022246282547712326\n",
      "Epoch 100, Loss: 0.012394175864756107\n",
      "Epoch 110, Loss: 0.006759673822671175\n",
      "Epoch 120, Loss: 0.0034875241108238697\n",
      "Epoch 130, Loss: 0.0017413144232705235\n",
      "Epoch 140, Loss: 0.0008235073764808476\n",
      "Epoch 150, Loss: 0.0003586069797165692\n",
      "Epoch 160, Loss: 0.0001706021575955674\n",
      "Epoch 170, Loss: 8.574590174248442e-05\n",
      "Epoch 180, Loss: 3.9966966141946614e-05\n",
      "Epoch 190, Loss: 1.586773396411445e-05\n"
     ]
    }
   ],
   "source": [
    "class ImprovedHyperGCN(tf.keras.Model):\n",
    "    def __init__(self, num_nodes, num_features, hidden_dims, output_dim, dropout_rate):\n",
    "        super(ImprovedHyperGCN, self).__init__()\n",
    "        self.embedding = layers.Embedding(num_nodes, num_features)\n",
    "        self.hidden_layers = [layers.Dense(dim, activation='relu', kernel_regularizer=regularizers.l2(0.01)) for dim in hidden_dims]\n",
    "        self.output_layer = layers.Dense(output_dim, activation='linear')\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, adj_matrix):\n",
    "        x = self.embedding(tf.range(adj_matrix.shape[0]))\n",
    "        x = tf.sparse.sparse_dense_matmul(adj_matrix, x)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "def train_hypergcn_model(H, train, test, scenario):\n",
    "    user_ids = train['userId'].unique()\n",
    "    movie_ids = train['movieId'].unique()\n",
    "\n",
    "    user_mapping = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "    movie_mapping = {movie_id: idx + len(user_ids) for idx, movie_id in enumerate(movie_ids)}\n",
    "\n",
    "    train['user_idx'] = train['userId'].map(user_mapping)\n",
    "    train['movie_idx'] = train['movieId'].map(movie_mapping)\n",
    "\n",
    "    test['user_idx'] = test['userId'].map(user_mapping)\n",
    "    test['movie_idx'] = test['movieId'].map(movie_mapping)\n",
    "\n",
    "    num_nodes = len(user_ids) + len(movie_ids)\n",
    "    num_features = 128\n",
    "    hidden_dims = [256, 128]\n",
    "    output_dim = 1\n",
    "    dropout_rate = 0.5\n",
    "\n",
    "    model = ImprovedHyperGCN(num_nodes, num_features, hidden_dims, output_dim, dropout_rate)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    def pairwise_hinge_loss(positive_scores, negative_scores, margin=1.0):\n",
    "        return tf.reduce_mean(tf.maximum(0.0, margin - positive_scores + negative_scores))\n",
    "\n",
    "    def improved_sample_negative_edges(train_df, user_mapping, movie_mapping, num_neg_samples):\n",
    "        users = train_df['userId'].unique()\n",
    "        movies = train_df['movieId'].unique()\n",
    "        positive_samples = train_df[['userId', 'movieId']].values\n",
    "        positive_samples_set = set((user_mapping[user], movie_mapping[movie]) for user, movie in positive_samples)\n",
    "        \n",
    "        neg_samples = []\n",
    "        while len(neg_samples) < num_neg_samples:\n",
    "            user = np.random.choice(users)\n",
    "            user_idx = user_mapping[user]\n",
    "            negative_movies = np.setdiff1d(movies, train_df[train_df['userId'] == user]['movieId'].values)\n",
    "            neg_movie = np.random.choice(negative_movies)\n",
    "            neg_movie_idx = movie_mapping[neg_movie]\n",
    "            if (user_idx, neg_movie_idx) not in positive_samples_set:\n",
    "                neg_samples.append((user_idx, neg_movie_idx))\n",
    "        \n",
    "        neg_df = pd.DataFrame(neg_samples, columns=['user_idx', 'movie_idx'])\n",
    "        print(f\"Generated {len(neg_df)} negative samples\")\n",
    "        return neg_df\n",
    "\n",
    "    positive_samples = train[['user_idx', 'movie_idx']]\n",
    "    neg_train_samples = improved_sample_negative_edges(train, user_mapping, movie_mapping, num_neg_samples=len(positive_samples))\n",
    "    print(f\"Number of positive samples: {len(positive_samples)}\")\n",
    "    print(f\"Number of negative samples: {len(neg_train_samples)}\")\n",
    "\n",
    "    epochs = 200\n",
    "    batch_size = 256\n",
    "\n",
    "    def train_step(model, adj_matrix, optimizer, positive_samples, negative_samples):\n",
    "        with tf.GradientTape() as tape:\n",
    "            positive_user_embeddings = model.embedding(positive_samples['user_idx'].values)\n",
    "            positive_movie_embeddings = model.embedding(positive_samples['movie_idx'].values)\n",
    "            negative_user_embeddings = model.embedding(negative_samples['user_idx'].values)\n",
    "            negative_movie_embeddings = model.embedding(negative_samples['movie_idx'].values)\n",
    "\n",
    "            positive_scores = tf.reduce_sum(positive_user_embeddings * positive_movie_embeddings, axis=1)\n",
    "            negative_scores = tf.reduce_sum(negative_user_embeddings * negative_movie_embeddings, axis=1)\n",
    "\n",
    "            loss = pairwise_hinge_loss(positive_scores, negative_scores)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_step(model, adj_matrix_sparse, optimizer, positive_samples, neg_train_samples)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.numpy()}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    embeddings = model.embedding(tf.range(num_nodes)).numpy()\n",
    "\n",
    "    results = evaluate_model(test, embeddings, user_mapping, movie_mapping, scenario, \"HyperGCN\")\n",
    "    results['Running Time (s)'] = end_time - start_time\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate HyperGCN model for different scenarios\n",
    "results_hypergcn_normal = train_hypergcn_model(H, train_df, test_df, \"Normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fa90d43-a3d9-4ca9-99d8-187bf82df6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>Running Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>HyperGCN</td>\n",
       "      <td>5.608074</td>\n",
       "      <td>2.368137</td>\n",
       "      <td>2.05015</td>\n",
       "      <td>0.116924</td>\n",
       "      <td>0.009341</td>\n",
       "      <td>86.924503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Scenario Algorithm       MSE      RMSE      MAE  Precision@10  Recall@10  \\\n",
       "0   Normal  HyperGCN  5.608074  2.368137  2.05015      0.116924   0.009341   \n",
       "\n",
       "   Running Time (s)  \n",
       "0         86.924503  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_hypergcn_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c6901-ffcc-491a-b85f-38e41815bcd8",
   "metadata": {},
   "source": [
    "# Sparse data HyperGCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b6ab79d-7414-4450-ad0f-d616c84b69c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix created with shape (3794, 3794)\n",
      "Generated 8067 negative samples\n",
      "Number of positive samples: 8067\n",
      "Number of negative samples: 8067\n",
      "Epoch 0, Loss: 1.000066876411438\n",
      "Epoch 10, Loss: 0.959601104259491\n",
      "Epoch 20, Loss: 0.8961366415023804\n",
      "Epoch 30, Loss: 0.7816119194030762\n",
      "Epoch 40, Loss: 0.5799558162689209\n",
      "Epoch 50, Loss: 0.27321043610572815\n",
      "Epoch 60, Loss: 0.04732617735862732\n",
      "Epoch 70, Loss: 0.0027934403624385595\n",
      "Epoch 80, Loss: 2.025742651312612e-05\n",
      "Epoch 90, Loss: 0.0\n",
      "Epoch 100, Loss: 0.0\n",
      "Epoch 110, Loss: 0.0\n",
      "Epoch 120, Loss: 0.0\n",
      "Epoch 130, Loss: 0.0\n",
      "Epoch 140, Loss: 0.0\n",
      "Epoch 150, Loss: 0.0\n",
      "Epoch 160, Loss: 0.0\n",
      "Epoch 170, Loss: 0.0\n",
      "Epoch 180, Loss: 0.0\n",
      "Epoch 190, Loss: 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>Running Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sparse</td>\n",
       "      <td>HyperGCN</td>\n",
       "      <td>13.239963</td>\n",
       "      <td>3.638676</td>\n",
       "      <td>3.480585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.528216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Scenario Algorithm        MSE      RMSE       MAE  Precision@10  Recall@10  \\\n",
       "0   Sparse  HyperGCN  13.239963  3.638676  3.480585           0.0        0.0   \n",
       "\n",
       "   Running Time (s)  \n",
       "0         11.528216  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_train_df = get_sparse_data(train_df, frac=0.1)\n",
    "\n",
    "# Sparse \n",
    "# Build the hypergraph\n",
    "edges_sparse = defaultdict(list)\n",
    "for _, row in sparse_train_df.iterrows():\n",
    "    user_node = f'user_{row[\"userId\"]}'\n",
    "    movie_node = f'movie_{row[\"movieId\"]}'\n",
    "    rating = row[\"rating\"]\n",
    "    hyperedge = f'{movie_node}_rating_{rating}'\n",
    "    edges_sparse[hyperedge].append(user_node)\n",
    "    edges_sparse[hyperedge].append(movie_node)\n",
    "\n",
    "H_sparse = hnx.Hypergraph(edges_sparse)\n",
    "\n",
    "adj_matrix, node_to_idx = create_hypergraph_adjacency_matrix(H_sparse)\n",
    "adj_matrix_sparse = tf.sparse.SparseTensor(indices=np.array([adj_matrix.nonzero()[0], adj_matrix.nonzero()[1]]).T,\n",
    "                                           values=adj_matrix.data.astype(np.float32),\n",
    "                                           dense_shape=adj_matrix.shape)\n",
    "adj_matrix_sparse = tf.sparse.reorder(adj_matrix_sparse)\n",
    "\n",
    "\n",
    "results_hypergcn_sparse = train_hypergcn_model(H_sparse, sparse_train_df, test_df, \"Sparse\")\n",
    "results_hypergcn_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3680a-2ea1-4f24-9a12-25bf9edcbfa3",
   "metadata": {},
   "source": [
    "# New User HyperGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2f459c0-3032-42fd-be5e-415d731dfd10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix created with shape (9564, 9564)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/js5nvmg90d37xtyx33clr3th0000gn/T/ipykernel_2832/2788340750.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['user_idx'] = train['userId'].map(user_mapping)\n",
      "/var/folders/dr/js5nvmg90d37xtyx33clr3th0000gn/T/ipykernel_2832/2788340750.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['movie_idx'] = train['movieId'].map(movie_mapping)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 80184 negative samples\n",
      "Number of positive samples: 80184\n",
      "Number of negative samples: 80184\n",
      "Epoch 0, Loss: 0.9998919367790222\n",
      "Epoch 10, Loss: 0.9869893193244934\n",
      "Epoch 20, Loss: 0.9545198678970337\n",
      "Epoch 30, Loss: 0.8279085159301758\n",
      "Epoch 40, Loss: 0.4215397834777832\n",
      "Epoch 50, Loss: 0.19156445562839508\n",
      "Epoch 60, Loss: 0.11105109006166458\n",
      "Epoch 70, Loss: 0.06559161841869354\n",
      "Epoch 80, Loss: 0.037892796099185944\n",
      "Epoch 90, Loss: 0.021464204415678978\n",
      "Epoch 100, Loss: 0.01199073065072298\n",
      "Epoch 110, Loss: 0.0064708152785897255\n",
      "Epoch 120, Loss: 0.0033071571961045265\n",
      "Epoch 130, Loss: 0.0016385489143431187\n",
      "Epoch 140, Loss: 0.000772208790294826\n",
      "Epoch 150, Loss: 0.00035553640918806195\n",
      "Epoch 160, Loss: 0.00015765165153425187\n",
      "Epoch 170, Loss: 5.20551620866172e-05\n",
      "Epoch 180, Loss: 1.1776611245295499e-05\n",
      "Epoch 190, Loss: 9.506505875833682e-07\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>Running Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New User</td>\n",
       "      <td>HyperGCN</td>\n",
       "      <td>5.691181</td>\n",
       "      <td>2.38562</td>\n",
       "      <td>2.063831</td>\n",
       "      <td>0.105558</td>\n",
       "      <td>0.008382</td>\n",
       "      <td>88.79288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Scenario Algorithm       MSE     RMSE       MAE  Precision@10  Recall@10  \\\n",
       "0  New User  HyperGCN  5.691181  2.38562  2.063831      0.105558   0.008382   \n",
       "\n",
       "   Running Time (s)  \n",
       "0          88.79288  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_train_df = get_new_user_data(train_df, frac=0.1)\n",
    "\n",
    "# Build the hypergraph\n",
    "edges_new_user = defaultdict(list)\n",
    "for _, row in new_user_train_df.iterrows():\n",
    "    user_node = f'user_{row[\"userId\"]}'\n",
    "    movie_node = f'movie_{row[\"movieId\"]}'\n",
    "    rating = row[\"rating\"]\n",
    "    hyperedge = f'{movie_node}_rating_{rating}'\n",
    "    edges_new_user[hyperedge].append(user_node)\n",
    "    edges_new_user[hyperedge].append(movie_node)\n",
    "\n",
    "H_new_user = hnx.Hypergraph(edges_new_user)\n",
    "\n",
    "adj_matrix, node_to_idx = create_hypergraph_adjacency_matrix(H_new_user)\n",
    "adj_matrix_sparse = tf.sparse.SparseTensor(indices=np.array([adj_matrix.nonzero()[0], adj_matrix.nonzero()[1]]).T,\n",
    "                                           values=adj_matrix.data.astype(np.float32),\n",
    "                                           dense_shape=adj_matrix.shape)\n",
    "adj_matrix_sparse = tf.sparse.reorder(adj_matrix_sparse)\n",
    "\n",
    "\n",
    "\n",
    "results_hypergcn_new_user = train_hypergcn_model(H_new_user, new_user_train_df, test_df, \"New User\")\n",
    "results_hypergcn_new_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e6d04c2-0b82-444b-9cd0-2f39a445d8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>Running Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>HyperGCN</td>\n",
       "      <td>5.608074</td>\n",
       "      <td>2.368137</td>\n",
       "      <td>2.050150</td>\n",
       "      <td>0.116924</td>\n",
       "      <td>0.009341</td>\n",
       "      <td>86.924503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sparse</td>\n",
       "      <td>HyperGCN</td>\n",
       "      <td>13.239963</td>\n",
       "      <td>3.638676</td>\n",
       "      <td>3.480585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.528216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New User</td>\n",
       "      <td>HyperGCN</td>\n",
       "      <td>5.691181</td>\n",
       "      <td>2.385620</td>\n",
       "      <td>2.063831</td>\n",
       "      <td>0.105558</td>\n",
       "      <td>0.008382</td>\n",
       "      <td>88.792880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Scenario Algorithm        MSE      RMSE       MAE  Precision@10  Recall@10  \\\n",
       "0    Normal  HyperGCN   5.608074  2.368137  2.050150      0.116924   0.009341   \n",
       "1    Sparse  HyperGCN  13.239963  3.638676  3.480585      0.000000   0.000000   \n",
       "2  New User  HyperGCN   5.691181  2.385620  2.063831      0.105558   0.008382   \n",
       "\n",
       "   Running Time (s)  \n",
       "0         86.924503  \n",
       "1         11.528216  \n",
       "2         88.792880  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine HyperGCN results into a single DataFrame\n",
    "results_hypergcn_combined = pd.concat([results_hypergcn_normal, results_hypergcn_sparse, results_hypergcn_new_user], ignore_index=True)\n",
    "results_hypergcn_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6829ec-72b1-4d96-a9a8-8cef489db946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
